import sys
from nltk.tokenize import word_tokenize

print("This program generates random sentences based on an Ngram model.")
for x in range(len(sys.argv[1:])):
    if(x==0):
        print("Ngrams= "+sys.argv[1])
        ngrams = sys.argv[1]
    elif(x==1):
        print("Number of sentences= "+sys.argv[2])
        sentences = sys.argv[2]
    else:
        print("File no."+ str(x-1)+"= "+ sys.argv[x+1])

#reading all the files and extracting tokens
tokens=[]
for y in range(2,len(sys.argv[1:])):
    with open(sys.argv[y+1]) as f_open:
        tokens.extend(word_tokenize(f_open.read()))

print(tokens)
